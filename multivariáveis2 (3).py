# -*- coding: utf-8 -*-
"""Multivari√°veis2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/168-aG1Y5Fe9pewTfSvwRQl-lkOYQcY5C
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

df_loaded = pd.read_csv('dados_campanha_vest - dados_campanha_vest (1).csv')

df.shape

print(df.info())
print(df.isnull().sum())

print("Colunas encontradas no CSV:")
print(df.columns)

df = df.rename(columns=lambda x: x.strip())

print(df)

df = df.rename(columns={
    'Custo': 'investimento',
    'CPL': 'cpl',
    'Cliques': 'cliques',
    'Impr.': 'impressoes',
    'CPC m√©d.': 'cpc_medio',
    'CTR': 'ctr',
    'Convers√µes': 'leads',
    'Sexo': 'sexo',
    'Dispositivo': 'dispositivo',
    'Idade': 'idade',
    'Tipo de campanha': 'tipo_de_campanha'
})

print("Colunas ap√≥s renomear:")
print(df.columns)

colunas_necessarias = ['investimento', 'ctr', 'cpl', 'taxa_conv', 'cliques',
                       'impressoes', 'cpc_medio', 'leads', 'sexo', 'dispositivo',
                       'idade', 'tipo_de_campanha']

df_encoded = pd.get_dummies(df, columns=['sexo', 'dispositivo', 'idade', 'tipo_de_campanha'], drop_first=True)

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np
import matplotlib.pyplot as plt
import xgboost as xgb

# Converter v√≠rgulas para ponto e transformar em float
colunas_numericas = ['investimento', 'ctr', 'cpl','cliques', 'impressoes', 'cpc_medio', 'leads']

colunas_numericas = ['investimento', 'ctr', 'cpl', 'cliques', 'impressoes', 'cpc_medio', 'leads']

df_encoded = pd.get_dummies(df, columns=['sexo', 'dispositivo', 'idade', 'tipo_de_campanha'], drop_first=True)

X = df_encoded[['investimento', 'ctr', 'cpl', 'cliques', 'impressoes', 'cpc_medio'] + [col for col in df_encoded.columns if any(cat in col for cat in ['sexo_', 'dispositivo_', 'idade_', 'tipo_de_campanha_'])]]

y = df_encoded['leads']

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import xgboost as xgb

import numpy as np
from sklearn.metrics import r2_score, mean_squared_error

import numpy as np

y = np.nan_to_num(y, nan=0)

df = df.fillna(0)

for col in colunas_para_converter_para_float:
    if col in df.columns: # Garante que a coluna existe
        # 1. Converte para string para usar m√©todos .str
        # 2. Remove o s√≠mbolo de porcentagem, se houver
        # 3. Remove o separador de milhares (ponto '.')
        # 4. Converte a v√≠rgula ',' para ponto '.' (separador decimal padr√£o do Python)
        # 5. Remove espa√ßos em branco extras
        df[col] = (
            df[col]
            .astype(str)
            .str.replace('%', '', regex=False)
            .str.replace('.', '', regex=False)
            .str.replace(',', '.', regex=False)
            .str.strip()
        )

print("Tamanho de y_test original:", len(y_test_imputed))
print("Quantos NaNs em y_test:", np.isnan(y_test_imputed).sum())
print("Quantos valores v√°lidos em y_test:", np.sum(~np.isnan(y_test_imputed)))

# X e y para calcular a correla√ß√£o facilmente
df_para_correlacao = pd.concat([X_train_imputed, y_train], axis=1) # Usando os dados de treino limpos

print("--- Matriz de Correla√ß√£o com 'leads' (Vari√°veis Num√©ricas) ---")
# Calcule a correla√ß√£o de todas as colunas num√©ricas com 'leads'
correlacoes_com_leads = df_para_correlacao.corr()['leads'].sort_values(ascending=False)
print(correlacoes_com_leads)

# Opcional: Visualizar a matriz de correla√ß√£o completa com um mapa de calor
import seaborn as sns
plt.figure(figsize=(10, 8))
sns.heatmap(df_para_correlacao.corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Matriz de Correla√ß√£o das Features e Leads")
plt.show()

# Use o df_final_para_analise que j√° tem as colunas categ√≥ricas originais
# e os leads reais.

print("\n--- M√©dia de Leads por 'Sexo' ---")
print(df_final_para_analise.groupby('sexo')['leads'].mean().round(2))

print("\n--- M√©dia de Leads por 'Tipo de Campanha' ---")
print(df_final_para_analise.groupby('tipo_de_campanha')['leads'].mean().round(2))

print("\n--- M√©dia de Leads por 'Dispositivo' ---")
print(df_final_para_analise.groupby('dispositivo')['leads'].mean().round(2))

# Fa√ßa isso para todas as suas colunas categ√≥ricas.

# Ap√≥s treinar o modelo_xgb_otimizado (se voc√™ usou GridSearchCV) ou modelo_xgb
# Voc√™ precisa ter X_train_imputed e y_train do seu √∫ltimo passo correto.

# Treine o modelo XGBoost novamente (se ainda n√£o o fez no c√≥digo corrigido)
modelo_xgb = xgb.XGBRegressor(random_state=42, n_estimators=100)
modelo_xgb.fit(X_train_imputed, y_train)

print("\n--- Import√¢ncia das Features (XGBoost) ---")
feature_importances = pd.Series(modelo_xgb.feature_importances_, index=X_train_imputed.columns)
print(feature_importances.sort_values(ascending=False))

# Visualizar a import√¢ncia das features
plt.figure(figsize=(10, 6))
feature_importances.sort_values(ascending=False).plot(kind='bar')
plt.title("Import√¢ncia das Features no Modelo XGBoost")
plt.ylabel("Pontua√ß√£o de Import√¢ncia")
plt.tight_layout()
plt.show()

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train_imputed),
                               columns=X_train_imputed.columns,
                               index=X_train_imputed.index)

X_test_scaled = pd.DataFrame(scaler.transform(X_test_imputed),
                              columns=X_test_imputed.columns,
                              index=X_test_imputed.index)

# Agora treine seus modelos com X_train_scaled e X_test_scaled
# Exemplo:
modelo_lr_scaled = LinearRegression()
modelo_lr_scaled.fit(X_train_scaled, y_train)
y_pred_lr_scaled = modelo_lr_scaled.predict(X_test_scaled)
r2_lr_scaled = r2_score(y_test, y_pred_lr_scaled)
rmse_lr_scaled = np.sqrt(mean_squared_error(y_test, y_pred_lr_scaled))
print(f"\nRegress√£o Linear (escalada) - R¬≤: {r2_lr_scaled:.4f}, RMSE: {rmse_lr_scaled:.2f}")

# E o XGBoost tamb√©m pode se beneficiar ligeiramente ou ter otimiza√ß√£o mais r√°pida com dados escalados
modelo_xgb_scaled = xgb.XGBRegressor(random_state=42, n_estimators=100) # Use os melhores params se j√° tiver feito tuning
modelo_xgb_scaled.fit(X_train_scaled, y_train)
y_pred_xgb_scaled = modelo_xgb_scaled.predict(X_test_scaled)
r2_xgb_scaled = r2_score(y_test, y_pred_xgb_scaled)
rmse_xgb_scaled = np.sqrt(mean_squared_error(y_test, y_pred_xgb_scaled))
print(f"XGBoost (escalado) - R¬≤: {r2_xgb_scaled:.4f}, RMSE: {rmse_xgb_scaled:.2f}")

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import xgboost as xgb

# Suponha que 'df' √© o seu DataFrame carregado do arquivo original.

# --- PASSO 1: LIMPEZA E CONVERS√ÉO DE TIPOS NO DATAFRAME ORIGINAL (df) ---
# Este bloco deve vir logo ap√≥s o carregamento do seu DataFrame 'df'.

# 1.1. Convers√£o de colunas num√©ricas para float, tratando formata√ß√£o e NaNs/infinitos
colunas_para_converter_para_float = [
    'investimento', 'ctr', 'cpl', 'cliques', 'impressoes', 'cpc_medio', 'leads'
    # Adicione 'idade' aqui se ela deve ser num√©rica
]

for col in colunas_para_converter_para_float:
    if col in df.columns:
        df[col] = (
            df[col]
            .astype(str)
            .str.replace('%', '', regex=False)
            .str.replace('.', '', regex=False) # Remove separador de milhares
            .str.replace(',', '.', regex=False) # Converte v√≠rgula decimal para ponto
            .str.strip()
        )
        df[col] = df[col].replace('', np.nan) # Converte strings vazias para NaN
        df[col] = pd.to_numeric(df[col], errors='coerce') # Converte para float, erro vira NaN
        df[col] = df[col].replace([np.inf, -np.inf], np.nan) # Garante que infinitos viram NaN

# 1.2. Tratar NaNs na vari√°vel alvo ('leads') removendo as linhas do df
print(f"NaNs em 'leads' antes de remover linhas: {df['leads'].isnull().sum()}")
df.dropna(subset=['leads'], inplace=True) # Remove linhas onde 'leads' √© NaN
print(f"NaNs em 'leads' depois de remover linhas: {df['leads'].isnull().sum()}")

# --- PASSO 2: CODIFICA√á√ÉO DE VARI√ÅVEIS CATEG√ìRICAS (One-Hot Encoding) ---
# Fa√ßa isso no DataFrame 'df' j√° limpo.
colunas_categoricas = ['sexo', 'dispositivo', 'idade', 'tipo_de_campanha', 'C√≥digo da moeda'] # Verifique seus nomes reais
df_processed = pd.get_dummies(df, columns=colunas_categoricas, drop_first=True) # drop_first evita multicolinearidade

# --- PASSO 3: SEPARAR FEATURES (X) E TARGET (y) ---
# Agora, X e y ser√£o criados a partir do df_processed, que j√° tem tudo em float e dummies.
feature_cols = [
    'investimento', 'ctr', 'cpl', 'cliques', 'impressoes', 'cpc_medio'
]
# Adicionar as colunas dummy geradas
feature_cols.extend([col for col in df_processed.columns if any(cat in col for cat in ['sexo_', 'dispositivo_', 'idade_', 'tipo_de_campanha_', 'C√≥digo da moeda_'])])

X = df_processed[feature_cols].copy()
y = df_processed['leads'].copy()

# --- PASSO 4: DIVIDIR OS DADOS EM TREINO E TESTE ---
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=30)

# --- PASSO 5: IMPUTAR VALORES AUSENTES APENAS NOS CONJUNTOS DE TREINO/TESTE ---
# Este √© o ponto cr√≠tico para evitar vazamento de dados.
imputer = SimpleImputer(strategy='mean')

# Identificar colunas num√©ricas que podem ter NaNs em X (features)
colunas_numericas_X = X.select_dtypes(include=np.number).columns

# Fit do imputer APENAS no conjunto de TREINO e Transform em ambos
X_train_imputed = pd.DataFrame(imputer.fit_transform(X_train[colunas_numericas_X]),
                               columns=colunas_numericas_X, index=X_train.index)
X_test_imputed = pd.DataFrame(imputer.transform(X_test[colunas_numericas_X]),
                              columns=colunas_numericas_X, index=X_test.index)

# Verificar NaNs finais (deve ser zero)
print("\n--- Verifica√ß√£o de NaNs FINAIS antes do treinamento ---")
print("NaNs em X_train_imputed (total):", X_train_imputed.isnull().sum().sum())
print("NaNs em X_test_imputed (total):", X_test_imputed.isnull().sum().sum())
print("NaNs em y_train (total):", y_train.isnull().sum())
print("NaNs em y_test (total):", y_test.isnull().sum())

# --- PASSO 6: TREINAR E AVALIAR OS MODELOS ---

# Modelo de Regress√£o Linear
modelo_lr = LinearRegression()
modelo_lr.fit(X_train_imputed, y_train)
y_pred_lr = modelo_lr.predict(X_test_imputed)
r2_lr = r2_score(y_test, y_pred_lr)
rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))
print("\nüîé Avalia√ß√£o do Modelo de Regress√£o Linear:\n")
print(f"üìà R¬≤: {r2_lr:.4f}, RMSE: {rmse_lr:.2f}")

# Modelo XGBoost
modelo_xgb = xgb.XGBRegressor(random_state=42, n_estimators=100)
modelo_xgb.fit(X_train_imputed, y_train)
y_pred_xgb = modelo_xgb.predict(X_test_imputed)
r2_xgb = r2_score(y_test, y_pred_xgb)
rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))
print("\nüîé Avalia√ß√£o do Modelo XGBoost:\n")
print(f"üå≤ R¬≤: {r2_xgb:.4f}, RMSE: {rmse_xgb:.2f}")

# --- PASSO 7: RECONSTRUIR DATAFRAME PARA VISUALIZA√á√ÉO E AN√ÅLISE COMPARATIVA ---
# CUIDADO: Este bloco de reconstru√ß√£o √© apenas para visualiza√ß√£o e n√£o para treinar novamente.
# Ele recombina os dados treinados e testados para plotagem.

# Recombinar os dados de treino e teste imputados
X_completo_imputed = pd.concat([X_train_imputed, X_test_imputed], axis=0)
y_completo = pd.concat([y_train, y_test], axis=0)

# Alinhar os √≠ndices para garantir que X e y correspondem
X_completo_imputed = X_completo_imputed.sort_index()
y_completo = y_completo.sort_index()

# Criar o DataFrame final para an√°lise, incluindo o target real
df_final_para_analise = pd.concat([X_completo_imputed, y_completo.rename('leads')], axis=1)

# Adicionar as colunas categ√≥ricas originais (n√£o-dummy) ao DataFrame de an√°lise para agrupamento
# Reindexamos para garantir que as linhas correspondem ap√≥s as remo√ß√µes de NaN em 'leads'
for cat_col in colunas_categoricas:
    df_final_para_analise[cat_col] = df[cat_col].reindex(df_final_para_analise.index)

# Adicionar previs√µes para todo o dataset (usando os modelos treinados)
df_final_para_analise['pred_lr'] = modelo_lr.predict(X_completo_imputed)
df_final_para_analise['pred_xgb'] = modelo_xgb.predict(X_completo_imputed)

# Agrupar e plotar por sexo
comparativo_por_sexo = df_final_para_analise.groupby('sexo')[['leads', 'pred_lr', 'pred_xgb']].sum().round(1)
print("\nüìä Comparativo por sexo (leads reais vs previstos):")
display(comparativo_por_sexo)

comparativo_por_sexo.plot(kind='bar', figsize=(8,5))
plt.title("üìä Leads reais vs. previstos por Sexo")
plt.ylabel("Total de Leads")
plt.xticks(rotation=0)
plt.legend(["Reais", "Regress√£o Linear", "XGBoost"])
plt.tight_layout()
plt.show()

# Agrupar e plotar por tipo de campanha
comparativo_por_tipo = df_final_para_analise.groupby('tipo_de_campanha')[['leads', 'pred_lr', 'pred_xgb']].sum().round(1)
print("\nüìä Comparativo por tipo de campanha (leads reais vs previstos):")
display(comparativo_por_tipo)

comparativo_por_tipo.plot(kind='bar', figsize=(10,5))
plt.title("üìä Leads reais vs. previstos por Tipo de Campanha")
plt.ylabel("Total de Leads")
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# --- Certifique-se de que estas vari√°veis est√£o dispon√≠veis do seu ambiente ---
# X_train_imputed: Features de treino (imputadas e possivelmente escaladas, dependendo do seu modelo final)
# modelo_lr: Seu modelo de Regress√£o Linear treinado
# df: Seu DataFrame original, limpo de NaNs na coluna 'leads' e com colunas convertidas
# scaler: Seu StandardScaler treinado, SE VOC√ä APLICOU ESCALONAMENTO EM X ANTES DE TREINAR modelo_lr
# (descomente e ajuste a linha do scaler.transform(df_previsao) se usou)
# np.expm1: Se voc√™ aplicou np.log1p na sua vari√°vel 'leads' (y_train), (descomente a linha np.expm1 nas previs√µes)

# --- Configura√ß√£o da Simula√ß√£o ---
novo_investimento = 200000

# 1. Calcular as m√©dias dos KPIs por 'tipo_de_campanha' do DataFrame original ou do X_train
# √â mais preciso usar o df_processed que cont√©m as colunas num√©ricas originais e a coluna categ√≥rica 'tipo_de_campanha'
# antes das dummies. Se a coluna 'tipo_de_campanha' n√£o est√° em df_processed (foi dummy), use o df original.
# Vou assumir que 'df' cont√©m as colunas originais e j√° est√° limpo.

kpi_cols = ['ctr', 'cpl', 'cliques', 'impressoes', 'cpc_medio']
# Calcular m√©dias por tipo de campanha
kpis_medios_por_campanha = df.groupby('tipo_de_campanha')[kpi_cols].mean().to_dict('index')

# Obter as categorias √∫nicas de 'sexo' e 'tipo_de_campanha'
sexo_categorias = sorted(df['sexo'].dropna().unique())
tipo_campanha_categorias = sorted(df['tipo_de_campanha'].dropna().unique())

resultados_simulacao = []

# --- Loop para Gerar Previs√µes para Cada Combina√ß√£o ---
for tipo_original in tipo_campanha_categorias:
    # Obter os KPIs m√©dios para o tipo de campanha atual
    # Se o tipo_original n√£o estiver em kpis_medios_por_campanha (e.g., categoria com poucos dados),
    # pode-se optar por usar as m√©dias gerais, mas idealmente, todos os tipos deveriam ter dados.
    kpis_atuais = kpis_medios_por_campanha.get(tipo_original, {})

    # Para cada sexo (dentro do tipo de campanha)
    for sexo_original in sexo_categorias:
        # Inicializar uma linha de dados para previs√£o com zeros, com base nas colunas do X_train_imputed
        data_row = {col: 0 for col in X_train_imputed.columns}

        # Definir o novo investimento
        data_row['investimento'] = novo_investimento

        # Preencher com os KPIs m√©dios ESPEC√çFICOS para o tipo de campanha atual
        for kpi_col in kpi_cols:
            if kpi_col in data_row and kpi_col in kpis_atuais:
                data_row[kpi_col] = kpis_atuais[kpi_col]
            elif kpi_col in data_row:
                # Fallback: Se n√£o h√° KPI m√©dio espec√≠fico (raro se 'df' √© completo), use a m√©dia geral
                data_row[kpi_col] = X_train_imputed[kpi_col].mean()

        # Ativar a dummy correspondente ao tipo de campanha atual
        tipo_dummy_col = f'tipo_de_campanha_{tipo_original}'
        if tipo_dummy_col in data_row:
            data_row[tipo_dummy_col] = 1
        # Se for a categoria base (que foi dropada pelo drop_first=True), suas dummies permanecer√£o 0.

        # Ativar a dummy correspondente ao sexo atual
        sexo_dummy_col = f'sexo_{sexo_original}'
        if sexo_dummy_col in data_row:
            data_row[sexo_dummy_col] = 1
        # O mesmo para a categoria base de sexo.

        # Criar DataFrame para previs√£o com a linha √∫nica
        df_previsao = pd.DataFrame([data_row])

        # Garantir que as colunas e a ordem s√£o id√™nticas √†s de X_train_imputed
        df_previsao = df_previsao[X_train_imputed.columns]

        # --- Etapa de Escalonamento (Descomente se voc√™ aplicou StandardScaler) ---
        # Se voc√™ escalou X_train_imputed antes de treinar modelo_lr, DESCOMENTE as linhas abaixo:
        # df_previsao_final = pd.DataFrame(scaler.transform(df_previsao), columns=df_previsao.columns)
        # previsao_bruta = modelo_lr.predict(df_previsao_final)[0]

        # --- Etapa SEM Escalonamento (Use se n√£o aplicou StandardScaler) ---
        previsao_bruta = modelo_lr.predict(df_previsao)[0]

        # --- Reverter Logaritmo (Descomente se voc√™ aplicou np.log1p em y_train) ---
        # Se voc√™ transformou 'leads' com np.log1p, DESCOMENTE a linha abaixo:
        # previsao_final = np.expm1(previsao_bruta)
        # Caso contr√°rio:
        previsao_final = previsao_bruta

        # Arredondar e garantir que a previs√£o n√£o seja negativa
        leads_previstos = max(0, round(previsao_final, 0))

        resultados_simulacao.append({
            'Tipo de Campanha': tipo_original,
            'Sexo': sexo_original,
            'Investimento (R$)': novo_investimento,
            'Leads Previstos': leads_previstos
        })

df_resultados_simulacao_especifica = pd.DataFrame(resultados_simulacao)

print(f"\n--- Tabela Completa de Leads Previstos (Investimento: R$ {novo_investimento:,.2f}) ---")
print("  (Com KPIs m√©dios espec√≠ficos por Tipo de Campanha)")
display(df_resultados_simulacao_especifica)

plt.figure(figsize=(12, 7))
sns.barplot(data=df_resultados_simulacao_especifica, x='Tipo de Campanha', y='Leads Previstos', hue='Sexo', palette='viridis')
plt.title(f"Leads Previstos por Tipo de Campanha e Sexo (Investimento: R$ {novo_investimento:,.2f})")
plt.xlabel("Tipo de Campanha")
plt.ylabel("Total de Leads Previstos")
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

df_total_por_campanha_especifica = df_resultados_simulacao_especifica.groupby('Tipo de Campanha')['Leads Previstos'].sum().reset_index()
print(f"\n--- Total de Leads Previstos por Tipo de Campanha (Investimento: R$ {novo_investimento:,.2f}) ---")
print("  (Com KPIs m√©dios espec√≠ficos por Tipo de Campanha)")
display(df_total_por_campanha_especifica)

plt.figure(figsize=(10, 6))
sns.barplot(data=df_total_por_campanha_especifica, x='Tipo de Campanha', y='Leads Previstos', palette='coolwarm')
plt.title(f"Total de Leads Previstos por Tipo de Campanha (Investimento: R$ {novo_investimento:,.2f})")
plt.xlabel("Tipo de Campanha")
plt.ylabel("Total de Leads Previstos")
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

from sklearn.metrics import r2_score
import numpy as np

# Suponha que y_test, y_pred_lr e y_pred_xgb_tuned (ou y_pred_xgb)
# s√£o as vari√°veis com seus valores reais e previstos no conjunto de teste.
# Certifique-se de que y_pred_lr e y_pred_xgb est√£o na escala original de leads
# (se voc√™ usou np.log1p para transformar y_train, lembre-se de np.expm1 nas previs√µes).

# --- Para o Modelo de Regress√£o Linear ---
r2_lr = r2_score(y_test, y_pred_lr)
rmse_lr = np.sqrt(mean_squared_error(y_test, y_pred_lr))

print("üîé Avalia√ß√£o do Modelo de Regress√£o Linear:\n")
print(f"üìà R¬≤: {r2_lr:.4f}, RMSE: {rmse_lr:.2f}")

# --- Para o Modelo XGBoost ---
r2_xgb = r2_score(y_test, y_pred_xgb) # Use y_pred_xgb_tuned se tiver ajustado
rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))

print("\nüîé Avalia√ß√£o do Modelo XGBoost:\n")
print(f"üå≤ R¬≤: {r2_xgb:.4f}, RMSE: {rmse_xgb:.2f}")

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler # Descomente se voc√™ usou StandardScaler
import joblib # Para salvar/carregar modelos e scalers

# --- Configura√ß√µes Iniciais do Streamlit ---
st.set_page_config(layout="wide", page_title="Simulador de Leads por Investimento")
st.title("üìä Simulador de Leads por Investimento")
st.markdown("Insira o valor do investimento para prever os leads por sexo e tipo de campanha.")

# --- Fun√ß√µes de Cache para Carregamento/Processamento de Dados e Modelo ---
# Para um aplicativo real, voc√™ salvaria e carregaria seu modelo e scalers.
# Para este exemplo, vamos simular o carregamento e o pr√©-processamento inicial.

@st.cache_resource # Usar st.cache_resource para objetos de modelo
def load_model():
    """Carrega o modelo de Regress√£o Linear treinado."""
    # Substitua esta parte pelo seu modelo_lr treinado
    # Exemplo: modelo = joblib.load('caminho/para/seu/modelo_lr.pkl')
    # Ou, se voc√™ estiver rodando em um ambiente onde modelo_lr j√° est√° na mem√≥ria:
    # return modelo_lr # Apenas se modelo_lr j√° foi treinado e est√° acess√≠vel

    # --- SIMULA√á√ÉO: TREINAMENTO DO MODELO (APENAS PARA DEMONSTRA√á√ÉO NO APP) ---
    # EM UM APP REAL, VOC√ä CARREGARIA O MODELO J√Å TREINADO, N√ÉO TREINARIA AQUI.
    # Este bloco √© um placeholder. Voc√™ deve substitu√≠-lo pelo carregamento do seu modelo_lr.
    try:
        # Tenta usar o modelo_lr da sess√£o anterior (se em Jupyter/Colab)
        if 'modelo_lr' in globals():
            st.write("Usando modelo_lr existente na mem√≥ria.")
            return globals()['modelo_lr']
        else:
            st.error("Modelo 'modelo_lr' n√£o encontrado na mem√≥ria. Por favor, certifique-se de que o modelo foi treinado e est√° dispon√≠vel.")
            st.stop() # Para a execu√ß√£o do Streamlit
    except NameError:
        st.error("Modelo 'modelo_lr' n√£o encontrado. Por favor, treine seu modelo e execute novamente.")
        st.stop()
    # --- FIM DA SIMULA√á√ÉO ---

@st.cache_data # Usar st.cache_data para dados e resultados de pr√©-processamento
def load_and_preprocess_data():
    """
    Carrega e pr√©-processa o DataFrame.
    EM UM APP REAL, VOC√ä CARREGARIA SEU DF LIMPO E PR√â-PROCESSADO.
    """
    # Substitua esta parte pelo carregamento do seu DataFrame 'df'
    # Exemplo: df_loaded = pd.read_csv('caminho/para/seu/dados_limpos.csv')
    # Ou, se voc√™ estiver rodando em um ambiente onde 'df' j√° est√° na mem√≥ria:
    # df_loaded = df.copy() # Apenas se 'df' j√° foi carregado e limpo

    # --- SIMULA√á√ÉO: CARREGAMENTO E PR√â-PROCESSAMENTO INICIAL DO DF ---
    # Este bloco √© um placeholder. Voc√™ deve substitu√≠-lo pelo carregamento do seu 'df'
    # e garantir que ele esteja no estado correto (sem NaNs em 'leads', colunas num√©ricas como float).
    try:
        if 'df' in globals():
            st.write("Usando DataFrame 'df' existente na mem√≥ria.")
            df_loaded = globals()['df'].copy()
        else:
            st.error("DataFrame 'df' n√£o encontrado na mem√≥ria. Por favor, certifique-se de que o DataFrame foi carregado e pr√©-processado.")
            st.stop()

        # Recalcular colunas dummies para garantir que X_train_imputed.columns est√° correto
        # Isso √© crucial se o Streamlit rodar de forma independente e n√£o tiver acesso ao df_encoded.
        colunas_categoricas = ['sexo', 'dispositivo', 'idade', 'tipo_de_campanha', 'C√≥digo da moeda']
        df_processed_for_dummies = pd.get_dummies(df_loaded, columns=colunas_categoricas, drop_first=True)

        # Assumindo que X_train_imputed.columns √© uma lista global ou pode ser carregada
        # Voc√™ deve substituir isso pela sua lista real de colunas de X_train_imputed
        # Exemplo: X_train_imputed_columns = joblib.load('caminho/para/suas/colunas_X.pkl')
        if 'X_train_imputed' in globals():
            X_train_imputed_columns = globals()['X_train_imputed'].columns
        else:
            st.error("X_train_imputed n√£o encontrado na mem√≥ria. Por favor, certifique-se de que o X_train_imputed foi criado.")
            st.stop()


        # Calcular KPIs m√©dios por tipo de campanha
        kpi_cols = ['ctr', 'cpl', 'cliques', 'impressoes', 'cpc_medio']
        kpis_medios_por_campanha = df_loaded.groupby('tipo_de_campanha')[kpi_cols].mean().to_dict('index')

        sexo_categorias = sorted(df_loaded['sexo'].dropna().unique())
        tipo_campanha_categorias = sorted(df_loaded['tipo_de_campanha'].dropna().unique())

        # Carregar o scaler se ele foi usado
        scaler = None
        # Exemplo: if 'scaler' in globals(): scaler = globals()['scaler']
        # Ou: scaler = joblib.load('caminho/para/seu/scaler.pkl')

        # Flag para reverter log transform em y
        log_transform_y = False # Mude para True se voc√™ usou np.log1p em y_train

        return df_loaded, df_processed_for_dummies, X_train_imputed_columns, kpis_medios_por_campanha, sexo_categorias, tipo_campanha_categorias, scaler, log_transform_y

    except NameError:
        st.error("Erro ao carregar dados. Verifique se 'df' e 'X_train_imputed' est√£o definidos.")
        st.stop()
    # --- FIM DA SIMULA√á√ÉO ---


# --- Fun√ß√£o de Predi√ß√£o Principal ---
def predict_leads_for_investment(
    novo_investimento_val,
    model,
    X_train_cols,
    kpis_by_campaign,
    sexo_cats,
    campaign_cats,
    df_for_dummies, # DataFrame para garantir que as dummies s√£o criadas corretamente
    scaler_obj=None,
    log_transform_y_flag=False
):
    """
    Gera previs√µes de leads para um novo investimento, discriminado por sexo e tipo de campanha,
    usando KPIs m√©dios espec√≠ficos por tipo de campanha.
    """
    resultados = []
    kpi_cols = ['ctr', 'cpl', 'cliques', 'impressoes', 'cpc_medio']

    for tipo_original in campaign_cats:
        kpis_atuais = kpis_by_campaign.get(tipo_original, {})

        for sexo_original in sexo_cats:
            # Inicializar uma linha de dados para previs√£o com zeros, com base nas colunas do modelo
            data_row = {col: 0 for col in X_train_cols}

            # Preencher com o novo investimento e os KPIs m√©dios espec√≠ficos
            data_row['investimento'] = novo_investimento_val
            for kpi_col in kpi_cols:
                if kpi_col in data_row and kpi_col in kpis_atuais:
                    data_row[kpi_col] = kpis_atuais[kpi_col]
                elif kpi_col in data_row:
                    # Fallback para m√©dia geral se n√£o houver KPI espec√≠fico (improv√°vel se dados completos)
                    # Isso exigiria acesso ao X_train_imputed original ou suas m√©dias globais
                    # Para simplificar, vamos usar 0 ou uma m√©dia global pr√©-calculada se kpi_col n√£o estiver em kpis_atuais
                    # Em um cen√°rio real, voc√™ teria um fallback mais robusto.
                    pass # Deixa como 0 ou usa a m√©dia global se a coluna n√£o for preenchida

            # Gerar dummies para a linha de previs√£o
            # Criar um df tempor√°rio para gerar as dummies corretamente para essa linha
            temp_df_for_dummies = pd.DataFrame({
                'sexo': [sexo_original],
                'tipo_de_campanha': [tipo_original]
            })
            # Adicionar outras colunas categ√≥ricas se houver (dispositivo, idade, C√≥digo da moeda)
            # e preencher com a categoria mais frequente ou uma base para elas
            for cat_col in ['dispositivo', 'idade', 'C√≥digo da moeda']:
                if cat_col in df_for_dummies.columns:
                    temp_df_for_dummies[cat_col] = df_for_dummies[cat_col].mode()[0] # Usar a moda como padr√£o

            # Gerar as dummies para esta linha
            temp_dummies = pd.get_dummies(temp_df_for_dummies, columns=['sexo', 'tipo_de_campanha', 'dispositivo', 'idade', 'C√≥digo da moeda'], drop_first=True)

            # Preencher a data_row com as dummies geradas
            for dummy_col in temp_dummies.columns:
                if dummy_col in data_row:
                    data_row[dummy_col] = temp_dummies[dummy_col].iloc[0]

            # Criar DataFrame para previs√£o (com uma √∫nica linha)
            df_previsao = pd.DataFrame([data_row])

            # Garantir que as colunas e a ordem s√£o id√™nticas √†s de X_train_cols
            df_previsao = df_previsao[X_train_cols]

            # Aplicar escalonamento se o scaler foi fornecido
            if scaler_obj:
                df_previsao_final = pd.DataFrame(scaler_obj.transform(df_previsao), columns=df_previsao.columns)
            else:
                df_previsao_final = df_previsao

            # Fazer a previs√£o
            previsao_bruta = model.predict(df_previsao_final)[0]

            # Reverter logaritmo se a flag estiver ativada
            if log_transform_y_flag:
                previsao_final = np.expm1(previsao_bruta)
            else:
                previsao_final = previsao_bruta

            # Arredondar e garantir que a previs√£o n√£o seja negativa
            leads_previstos = max(0, round(previsao_final, 0))

            resultados.append({
                'Tipo de Campanha': tipo_original,
                'Sexo': sexo_original,
                'Investimento (R$)': novo_investimento_val,
                'Leads Previstos': leads_previstos
            })

    return pd.DataFrame(resultados)


# --- Carregar Modelo e Dados (uma vez por sess√£o Streamlit) ---
modelo = load_model()
df_original, df_processed_for_dummies, X_train_imputed_columns, kpis_medios_por_campanha, sexo_categorias, tipo_campanha_categorias, scaler, log_transform_y = load_and_preprocess_data()


# --- Interface do Usu√°rio no Streamlit ---
st.sidebar.header("Configura√ß√µes de Simula√ß√£o")
novo_investimento_input = st.sidebar.number_input(
    "Insira o valor do Investimento (R$):",
    min_value=0.0,
    value=200000.0, # Valor padr√£o
    step=1000.0,
    format="%.2f"
)

if st.sidebar.button("Gerar Previs√µes"):
    if novo_investimento_input is not None:
        st.subheader(f"Resultados da Simula√ß√£o para Investimento de R$ {novo_investimento_input:,.2f}")

        # Gerar os resultados da simula√ß√£o
        df_simulacao_completa = predict_leads_for_investment(
            novo_investimento_input,
            modelo,
            X_train_imputed_columns,
            kpis_medios_por_campanha,
            sexo_categorias,
            tipo_campanha_categorias,
            df_original, # Passa o df_original para ajudar na cria√ß√£o das dummies
            scaler_obj=scaler, # Passa o scaler se usado
            log_transform_y_flag=log_transform_y # Passa a flag de log transform
        )

        # --- Exibir Tabela Completa ---
        st.write("### Tabela Completa de Leads Previstos por Sexo e Tipo de Campanha")
        st.dataframe(df_simulacao_completa)

        # --- Gr√°fico de Leads Previstos por Tipo de Campanha e Sexo ---
        st.write("### Gr√°fico: Leads Previstos por Tipo de Campanha e Sexo")
        fig1, ax1 = plt.subplots(figsize=(12, 7))
        sns.barplot(data=df_simulacao_completa, x='Tipo de Campanha', y='Leads Previstos', hue='Sexo', palette='viridis', ax=ax1)
        ax1.set_title(f"Leads Previstos por Tipo de Campanha e Sexo (Investimento: R$ {novo_investimento_input:,.2f})")
        ax1.set_xlabel("Tipo de Campanha")
        ax1.set_ylabel("Total de Leads Previstos")
        plt.xticks(rotation=45, ha='right')
        st.pyplot(fig1)

        # --- Tabela de Leads Previstos Agrupados por Tipo de Campanha ---
        st.write("### Tabela: Total de Leads Previstos por Tipo de Campanha")
        df_total_por_campanha = df_simulacao_completa.groupby('Tipo de Campanha')['Leads Previstos'].sum().reset_index()
        st.dataframe(df_total_por_campanha)

        # --- Gr√°fico de Leads Previstos Agrupados por Tipo de Campanha ---
        st.write("### Gr√°fico: Total de Leads Previstos por Tipo de Campanha")
        fig2, ax2 = plt.subplots(figsize=(10, 6))
        sns.barplot(data=df_total_por_campanha, x='Tipo de Campanha', y='Leads Previstos', palette='coolwarm', ax=ax2)
        ax2.set_title(f"Total de Leads Previstos por Tipo de Campanha (Investimento: R$ {novo_investimento_input:,.2f})")
        ax2.set_xlabel("Tipo de Campanha")
        ax2.set_ylabel("Total de Leads Previstos")
        plt.xticks(rotation=45, ha='right')
        st.pyplot(fig2)

        # --- Tabela de Leads Previstos Agrupados por Sexo ---
        st.write("### Tabela: Total de Leads Previstos por Sexo")
        df_total_por_sexo = df_simulacao_completa.groupby('Sexo')['Leads Previstos'].sum().reset_index()
        st.dataframe(df_total_por_sexo)

        # --- Gr√°fico de Leads Previstos Agrupados por Sexo ---
        st.write("### Gr√°fico: Total de Leads Previstos por Sexo")
        fig3, ax3 = plt.subplots(figsize=(8, 5))
        sns.barplot(data=df_total_por_sexo, x='Sexo', y='Leads Previstos', palette='plasma', ax=ax3)
        ax3.set_title(f"Total de Leads Previstos por Sexo (Investimento: R$ {novo_investimento_input:,.2f})")
        ax3.set_xlabel("Sexo")
        ax3.set_ylabel("Total de Leads Previstos")
        st.pyplot(fig3)

import joblib
import pandas as pd
import numpy as np

# --- No seu script de treinamento, ap√≥s todas as etapas de pr√©-processamento e treinamento ---

# 1. Salvar o modelo treinado (modelo_lr)
joblib.dump(modelo_lr, 'modelo_lr.pkl')

# 2. Salvar o DataFrame original limpo (df)
# Este 'df' deve ser o DataFrame ap√≥s a limpeza inicial de NaNs em 'leads'
# e a convers√£o de tipos (antes do pd.get_dummies e train_test_split).
# Ele √© usado para obter as categorias e calcular KPIs m√©dios.
joblib.dump(df, 'df_original_limpo.pkl')

# 3. Salvar as colunas de X_train_imputed (para garantir a ordem das features)
joblib.dump(X_train_imputed.columns.tolist(), 'X_columns.pkl')

# 4. Salvar o scaler (se voc√™ usou StandardScaler ou outro escalonador)
# Descomente a linha abaixo se voc√™ usou um scaler no seu pipeline de treinamento.
# joblib.dump(scaler, 'scaler.pkl')

# 5. Salvar a flag de transforma√ß√£o logar√≠tmica em y (se voc√™ usou np.log1p em 'leads')
# Descomente e ajuste a linha abaixo se voc√™ aplicou np.log1p em y_train.
# with open('log_transform_y_flag.txt', 'w') as f:
#     f.write(str(log_transform_y)) # log_transform_y seria True ou False